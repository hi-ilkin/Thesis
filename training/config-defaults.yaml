# dataset specific config
batch_size:
  desc: Size of each mini-batch. EB0 - 256 too big for Titan, EB7 - 32 is enough
  value: 128
load_pretrained:
  desc: load original pretrained model
  value: true
load_checkpoint:
  desc: continue from checkpoint
  value: false
use_chunks:
  desc: If true, load trainer from .npz chunks, else from images. If chunks enabled, dataloaders will be loaded every time
  value: false

# training configuration
project:
  value: DFDC-phase-7-celeb-test
model_name:
  desc: timm model name
  value: tf_efficientnet_b5_ns
run_name:
  desc: name displayed at wandb
  value: s69_incpv4_default_high
notes:
  desc: Add run specific notes here. Don't forget to delete when starting new run
  value: Testing only inception_V4
resume_from:
  desc: checkpoint to continue from. None if you want start from scracth
  value: None
epochs:
  desc: Number of epochs to train. Should be chunk size * epoch if chunks used
  value: 10
target_size:
  desc: Output size
  value: 2
output_weights:
  desc: output weights for inbalanced dataset
  value: None
device:
  desc: use of GPU or CPU
  value: cuda
gpus:
  value: 1
precision:
  desc: half or full precision training. 16 and 32 supported
  value: 16
accumulate_grad_batches:
  desc: Accumulates grads every k batches
  value: 1
limit_train_batches:
  desc: limiting training batches per epoch. float - percent, int - batch count. Default 1.0 (all batches)
  value: 1.0
swa:
  desc: stochastic weight avgeraging
  value: false

# optimizers
opt_name:
  desc: Name of optimizer - [Adam, SGD]
  value: Adam

# lr scheduler setup
lr_scheduler:
  desc: Name of the learning rate scheduler - [fixed, lronplateau, step, cyclic, cyclic2, CosineAnnealingWarmRestarts]
  value: fixed
lr_min:
  desc: minnimum lr rate
  value: 0.00001
lr_max:
  desc: maximum lr rate. Used by fixed and starting point for step LR
  value: 0.0001
lr_gamma:
  desc: multipler for learning rate scheduler. Also used by step LR
  value: 0.5

## Cyclic specific params
lr_mode:
  desc: Mode for cyclic LR
  value: triangular2
lr_step_size:
  desc: Number of training epochs in the increasing half of a cycle. Step LR also uses this
  value: 2

## CosineAnnealingWarmRestarts specific params
lr_t0:
  desc: Number of iterations for the first restart
  value: 5
lr_tmult:
  desc: A factor increases after a restart
  value: 1

## LROnPlateau specific
lr_factor:
  value: 0.5
lr_patience:
  value: 2
lr_threshold:
  value: 0.5

# other
val_freq:
  desc: Frequency of valdiation check with epoch
  value: 1
log_freq:
  desc: Frequency of logging in steps
  value: 10